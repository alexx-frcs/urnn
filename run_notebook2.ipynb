{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Done.\n",
      "Starting training...\n",
      "Initializing and training URNNs for one timestep...\n",
      "Network __init__ over. Number of trainable params= 3073\n",
      "Training network  lru_urnn ... timesteps= 100\n",
      "Starting training for lru_urnn\n",
      "NumEpochs:  10 |BatchSize:  50 |NumBatches:   600 \n",
      "\n",
      "Epoch Starting: 0 \n",
      "\n",
      "Epoch:   0 |Batch:   0 |TotalExamples:    50 |BatchLoss:   1.0740\n",
      "Epoch:   0 |Batch:  10 |TotalExamples:   550 |BatchLoss:   0.1624\n",
      "Epoch:   0 |Batch:  20 |TotalExamples:  1050 |BatchLoss:   0.1933\n",
      "Epoch:   0 |Batch:  30 |TotalExamples:  1550 |BatchLoss:   0.1745\n",
      "Epoch:   0 |Batch:  40 |TotalExamples:  2050 |BatchLoss:   0.1781\n",
      "Epoch:   0 |Batch:  50 |TotalExamples:  2550 |BatchLoss:   0.2596\n",
      "Epoch:   0 |Batch:  60 |TotalExamples:  3050 |BatchLoss:   0.1624\n",
      "Epoch:   0 |Batch:  70 |TotalExamples:  3550 |BatchLoss:   0.1407\n",
      "Epoch:   0 |Batch:  80 |TotalExamples:  4050 |BatchLoss:   0.1410\n",
      "Epoch:   0 |Batch:  90 |TotalExamples:  4550 |BatchLoss:   0.1630\n",
      "Epoch:   0 |Batch: 100 |TotalExamples:  5050 |BatchLoss:   0.1608\n",
      "Epoch:   0 |Batch: 110 |TotalExamples:  5550 |BatchLoss:   0.2055\n",
      "Epoch:   0 |Batch: 120 |TotalExamples:  6050 |BatchLoss:   0.1556\n",
      "Epoch:   0 |Batch: 130 |TotalExamples:  6550 |BatchLoss:   0.1574\n",
      "Epoch:   0 |Batch: 140 |TotalExamples:  7050 |BatchLoss:   0.2307\n",
      "Epoch:   0 |Batch: 150 |TotalExamples:  7550 |BatchLoss:   0.1480\n",
      "Epoch:   0 |Batch: 160 |TotalExamples:  8050 |BatchLoss:   0.2354\n",
      "Epoch:   0 |Batch: 170 |TotalExamples:  8550 |BatchLoss:   0.1307\n",
      "Epoch:   0 |Batch: 180 |TotalExamples:  9050 |BatchLoss:   0.1498\n",
      "Epoch:   0 |Batch: 190 |TotalExamples:  9550 |BatchLoss:   0.1627\n",
      "Epoch:   0 |Batch: 200 |TotalExamples: 10050 |BatchLoss:   0.1260\n",
      "Epoch:   0 |Batch: 210 |TotalExamples: 10550 |BatchLoss:   0.1310\n",
      "Epoch:   0 |Batch: 220 |TotalExamples: 11050 |BatchLoss:   0.1632\n",
      "Epoch:   0 |Batch: 230 |TotalExamples: 11550 |BatchLoss:   0.1462\n",
      "Epoch:   0 |Batch: 240 |TotalExamples: 12050 |BatchLoss:   0.2207\n",
      "Epoch:   0 |Batch: 250 |TotalExamples: 12550 |BatchLoss:   0.1584\n",
      "Epoch:   0 |Batch: 260 |TotalExamples: 13050 |BatchLoss:   0.1503\n",
      "Epoch:   0 |Batch: 270 |TotalExamples: 13550 |BatchLoss:   0.1863\n",
      "Epoch:   0 |Batch: 280 |TotalExamples: 14050 |BatchLoss:   0.1216\n",
      "Epoch:   0 |Batch: 290 |TotalExamples: 14550 |BatchLoss:   0.1599\n",
      "Epoch:   0 |Batch: 300 |TotalExamples: 15050 |BatchLoss:   0.1122\n",
      "Epoch:   0 |Batch: 310 |TotalExamples: 15550 |BatchLoss:   0.2163\n",
      "Epoch:   0 |Batch: 320 |TotalExamples: 16050 |BatchLoss:   0.2058\n",
      "Epoch:   0 |Batch: 330 |TotalExamples: 16550 |BatchLoss:   0.1620\n",
      "Epoch:   0 |Batch: 340 |TotalExamples: 17050 |BatchLoss:   0.1740\n",
      "Epoch:   0 |Batch: 350 |TotalExamples: 17550 |BatchLoss:   0.1141\n",
      "Epoch:   0 |Batch: 360 |TotalExamples: 18050 |BatchLoss:   0.1826\n",
      "Epoch:   0 |Batch: 370 |TotalExamples: 18550 |BatchLoss:   0.1200\n",
      "Epoch:   0 |Batch: 380 |TotalExamples: 19050 |BatchLoss:   0.1487\n",
      "Epoch:   0 |Batch: 390 |TotalExamples: 19550 |BatchLoss:   0.1327\n",
      "Epoch:   0 |Batch: 400 |TotalExamples: 20050 |BatchLoss:   0.1885\n",
      "Epoch:   0 |Batch: 410 |TotalExamples: 20550 |BatchLoss:   0.2187\n",
      "Epoch:   0 |Batch: 420 |TotalExamples: 21050 |BatchLoss:   0.1822\n",
      "Epoch:   0 |Batch: 430 |TotalExamples: 21550 |BatchLoss:   0.1637\n",
      "Epoch:   0 |Batch: 440 |TotalExamples: 22050 |BatchLoss:   0.1846\n",
      "Epoch:   0 |Batch: 450 |TotalExamples: 22550 |BatchLoss:   0.1738\n",
      "Epoch:   0 |Batch: 460 |TotalExamples: 23050 |BatchLoss:   0.1775\n",
      "Epoch:   0 |Batch: 470 |TotalExamples: 23550 |BatchLoss:   0.1503\n",
      "Epoch:   0 |Batch: 480 |TotalExamples: 24050 |BatchLoss:   0.2234\n",
      "Epoch:   0 |Batch: 490 |TotalExamples: 24550 |BatchLoss:   0.1522\n",
      "Epoch:   0 |Batch: 500 |TotalExamples: 25050 |BatchLoss:   0.1947\n",
      "Epoch:   0 |Batch: 510 |TotalExamples: 25550 |BatchLoss:   0.2014\n",
      "Epoch:   0 |Batch: 520 |TotalExamples: 26050 |BatchLoss:   0.1582\n",
      "Epoch:   0 |Batch: 530 |TotalExamples: 26550 |BatchLoss:   0.1790\n",
      "Epoch:   0 |Batch: 540 |TotalExamples: 27050 |BatchLoss:   0.1530\n",
      "Epoch:   0 |Batch: 550 |TotalExamples: 27550 |BatchLoss:   0.1590\n",
      "Epoch:   0 |Batch: 560 |TotalExamples: 28050 |BatchLoss:   0.2380\n",
      "Epoch:   0 |Batch: 570 |TotalExamples: 28550 |BatchLoss:   0.1944\n",
      "Epoch:   0 |Batch: 580 |TotalExamples: 29050 |BatchLoss:   0.1451\n",
      "Epoch:   0 |Batch: 590 |TotalExamples: 29550 |BatchLoss:   0.1809\n",
      "Epoch Over:   0 |MeanEpochLoss:   0.1766 |ValidationSetLoss:   0.1660 \n",
      "\n",
      "Epoch Starting: 1 \n",
      "\n",
      "Epoch:   1 |Batch:   0 |TotalExamples: 30050 |BatchLoss:   0.1177\n",
      "Epoch:   1 |Batch:  10 |TotalExamples: 30550 |BatchLoss:   0.1617\n",
      "Epoch:   1 |Batch:  20 |TotalExamples: 31050 |BatchLoss:   0.1814\n",
      "Epoch:   1 |Batch:  30 |TotalExamples: 31550 |BatchLoss:   0.1686\n",
      "Epoch:   1 |Batch:  40 |TotalExamples: 32050 |BatchLoss:   0.1661\n",
      "Epoch:   1 |Batch:  50 |TotalExamples: 32550 |BatchLoss:   0.2527\n",
      "Epoch:   1 |Batch:  60 |TotalExamples: 33050 |BatchLoss:   0.1496\n",
      "Epoch:   1 |Batch:  70 |TotalExamples: 33550 |BatchLoss:   0.1449\n",
      "Epoch:   1 |Batch:  80 |TotalExamples: 34050 |BatchLoss:   0.1409\n",
      "Epoch:   1 |Batch:  90 |TotalExamples: 34550 |BatchLoss:   0.1657\n",
      "Epoch:   1 |Batch: 100 |TotalExamples: 35050 |BatchLoss:   0.1635\n",
      "Epoch:   1 |Batch: 110 |TotalExamples: 35550 |BatchLoss:   0.1931\n",
      "Epoch:   1 |Batch: 120 |TotalExamples: 36050 |BatchLoss:   0.1528\n",
      "Epoch:   1 |Batch: 130 |TotalExamples: 36550 |BatchLoss:   0.1501\n",
      "Epoch:   1 |Batch: 140 |TotalExamples: 37050 |BatchLoss:   0.2161\n",
      "Epoch:   1 |Batch: 150 |TotalExamples: 37550 |BatchLoss:   0.1570\n",
      "Epoch:   1 |Batch: 160 |TotalExamples: 38050 |BatchLoss:   0.2304\n",
      "Epoch:   1 |Batch: 170 |TotalExamples: 38550 |BatchLoss:   0.1326\n",
      "Epoch:   1 |Batch: 180 |TotalExamples: 39050 |BatchLoss:   0.1516\n",
      "Epoch:   1 |Batch: 190 |TotalExamples: 39550 |BatchLoss:   0.1605\n",
      "Epoch:   1 |Batch: 200 |TotalExamples: 40050 |BatchLoss:   0.1163\n",
      "Epoch:   1 |Batch: 210 |TotalExamples: 40550 |BatchLoss:   0.1260\n",
      "Epoch:   1 |Batch: 220 |TotalExamples: 41050 |BatchLoss:   0.1574\n",
      "Epoch:   1 |Batch: 230 |TotalExamples: 41550 |BatchLoss:   0.1368\n",
      "Epoch:   1 |Batch: 240 |TotalExamples: 42050 |BatchLoss:   0.2212\n",
      "Epoch:   1 |Batch: 250 |TotalExamples: 42550 |BatchLoss:   0.1555\n",
      "Epoch:   1 |Batch: 260 |TotalExamples: 43050 |BatchLoss:   0.1490\n",
      "Epoch:   1 |Batch: 270 |TotalExamples: 43550 |BatchLoss:   0.1820\n",
      "Epoch:   1 |Batch: 280 |TotalExamples: 44050 |BatchLoss:   0.1215\n",
      "Epoch:   1 |Batch: 290 |TotalExamples: 44550 |BatchLoss:   0.1604\n",
      "Epoch:   1 |Batch: 300 |TotalExamples: 45050 |BatchLoss:   0.1142\n",
      "Epoch:   1 |Batch: 310 |TotalExamples: 45550 |BatchLoss:   0.2178\n",
      "Epoch:   1 |Batch: 320 |TotalExamples: 46050 |BatchLoss:   0.2077\n",
      "Epoch:   1 |Batch: 330 |TotalExamples: 46550 |BatchLoss:   0.1665\n",
      "Epoch:   1 |Batch: 340 |TotalExamples: 47050 |BatchLoss:   0.1688\n",
      "Epoch:   1 |Batch: 350 |TotalExamples: 47550 |BatchLoss:   0.1119\n",
      "Epoch:   1 |Batch: 360 |TotalExamples: 48050 |BatchLoss:   0.1814\n",
      "Epoch:   1 |Batch: 370 |TotalExamples: 48550 |BatchLoss:   0.1142\n",
      "Epoch:   1 |Batch: 380 |TotalExamples: 49050 |BatchLoss:   0.1446\n",
      "Epoch:   1 |Batch: 390 |TotalExamples: 49550 |BatchLoss:   0.1312\n",
      "Epoch:   1 |Batch: 400 |TotalExamples: 50050 |BatchLoss:   0.1904\n",
      "Epoch:   1 |Batch: 410 |TotalExamples: 50550 |BatchLoss:   0.2114\n",
      "Epoch:   1 |Batch: 420 |TotalExamples: 51050 |BatchLoss:   0.1794\n",
      "Epoch:   1 |Batch: 430 |TotalExamples: 51550 |BatchLoss:   0.1592\n",
      "Epoch:   1 |Batch: 440 |TotalExamples: 52050 |BatchLoss:   0.1875\n",
      "Epoch:   1 |Batch: 450 |TotalExamples: 52550 |BatchLoss:   0.1697\n",
      "Epoch:   1 |Batch: 460 |TotalExamples: 53050 |BatchLoss:   0.1788\n",
      "Epoch:   1 |Batch: 470 |TotalExamples: 53550 |BatchLoss:   0.1488\n",
      "Epoch:   1 |Batch: 480 |TotalExamples: 54050 |BatchLoss:   0.2260\n",
      "Epoch:   1 |Batch: 490 |TotalExamples: 54550 |BatchLoss:   0.1517\n",
      "Epoch:   1 |Batch: 500 |TotalExamples: 55050 |BatchLoss:   0.1962\n",
      "Epoch:   1 |Batch: 510 |TotalExamples: 55550 |BatchLoss:   0.2032\n",
      "Epoch:   1 |Batch: 520 |TotalExamples: 56050 |BatchLoss:   0.1578\n",
      "Epoch:   1 |Batch: 530 |TotalExamples: 56550 |BatchLoss:   0.1817\n",
      "Epoch:   1 |Batch: 540 |TotalExamples: 57050 |BatchLoss:   0.1526\n",
      "Epoch:   1 |Batch: 550 |TotalExamples: 57550 |BatchLoss:   0.1591\n",
      "Epoch:   1 |Batch: 560 |TotalExamples: 58050 |BatchLoss:   0.2366\n",
      "Epoch:   1 |Batch: 570 |TotalExamples: 58550 |BatchLoss:   0.1917\n",
      "Epoch:   1 |Batch: 580 |TotalExamples: 59050 |BatchLoss:   0.1430\n",
      "Epoch:   1 |Batch: 590 |TotalExamples: 59550 |BatchLoss:   0.1804\n",
      "Epoch Over:   1 |MeanEpochLoss:   0.1706 |ValidationSetLoss:   0.1660 \n",
      "\n",
      "Epoch Starting: 2 \n",
      "\n",
      "Epoch:   2 |Batch:   0 |TotalExamples: 60050 |BatchLoss:   0.1183\n",
      "Epoch:   2 |Batch:  10 |TotalExamples: 60550 |BatchLoss:   0.1616\n",
      "Epoch:   2 |Batch:  20 |TotalExamples: 61050 |BatchLoss:   0.1807\n",
      "Epoch:   2 |Batch:  30 |TotalExamples: 61550 |BatchLoss:   0.1660\n",
      "Epoch:   2 |Batch:  40 |TotalExamples: 62050 |BatchLoss:   0.1641\n",
      "Epoch:   2 |Batch:  50 |TotalExamples: 62550 |BatchLoss:   0.2500\n",
      "Epoch:   2 |Batch:  60 |TotalExamples: 63050 |BatchLoss:   0.1502\n",
      "Epoch:   2 |Batch:  70 |TotalExamples: 63550 |BatchLoss:   0.1453\n",
      "Epoch:   2 |Batch:  80 |TotalExamples: 64050 |BatchLoss:   0.1433\n",
      "Epoch:   2 |Batch:  90 |TotalExamples: 64550 |BatchLoss:   0.1653\n",
      "Epoch:   2 |Batch: 100 |TotalExamples: 65050 |BatchLoss:   0.1658\n",
      "Epoch:   2 |Batch: 110 |TotalExamples: 65550 |BatchLoss:   0.1915\n",
      "Epoch:   2 |Batch: 120 |TotalExamples: 66050 |BatchLoss:   0.1519\n",
      "Epoch:   2 |Batch: 130 |TotalExamples: 66550 |BatchLoss:   0.1460\n",
      "Epoch:   2 |Batch: 140 |TotalExamples: 67050 |BatchLoss:   0.2152\n",
      "Epoch:   2 |Batch: 150 |TotalExamples: 67550 |BatchLoss:   0.1577\n",
      "Epoch:   2 |Batch: 160 |TotalExamples: 68050 |BatchLoss:   0.2285\n",
      "Epoch:   2 |Batch: 170 |TotalExamples: 68550 |BatchLoss:   0.1325\n",
      "Epoch:   2 |Batch: 180 |TotalExamples: 69050 |BatchLoss:   0.1518\n",
      "Epoch:   2 |Batch: 190 |TotalExamples: 69550 |BatchLoss:   0.1594\n",
      "Epoch:   2 |Batch: 200 |TotalExamples: 70050 |BatchLoss:   0.1139\n",
      "Epoch:   2 |Batch: 210 |TotalExamples: 70550 |BatchLoss:   0.1252\n",
      "Epoch:   2 |Batch: 220 |TotalExamples: 71050 |BatchLoss:   0.1548\n",
      "Epoch:   2 |Batch: 230 |TotalExamples: 71550 |BatchLoss:   0.1351\n",
      "Epoch:   2 |Batch: 240 |TotalExamples: 72050 |BatchLoss:   0.2198\n",
      "Epoch:   2 |Batch: 250 |TotalExamples: 72550 |BatchLoss:   0.1548\n",
      "Epoch:   2 |Batch: 260 |TotalExamples: 73050 |BatchLoss:   0.1499\n",
      "Epoch:   2 |Batch: 270 |TotalExamples: 73550 |BatchLoss:   0.1796\n",
      "Epoch:   2 |Batch: 280 |TotalExamples: 74050 |BatchLoss:   0.1209\n",
      "Epoch:   2 |Batch: 290 |TotalExamples: 74550 |BatchLoss:   0.1615\n",
      "Epoch:   2 |Batch: 300 |TotalExamples: 75050 |BatchLoss:   0.1148\n",
      "Epoch:   2 |Batch: 310 |TotalExamples: 75550 |BatchLoss:   0.2177\n",
      "Epoch:   2 |Batch: 320 |TotalExamples: 76050 |BatchLoss:   0.2079\n",
      "Epoch:   2 |Batch: 330 |TotalExamples: 76550 |BatchLoss:   0.1667\n",
      "Epoch:   2 |Batch: 340 |TotalExamples: 77050 |BatchLoss:   0.1678\n",
      "Epoch:   2 |Batch: 350 |TotalExamples: 77550 |BatchLoss:   0.1107\n",
      "Epoch:   2 |Batch: 360 |TotalExamples: 78050 |BatchLoss:   0.1808\n",
      "Epoch:   2 |Batch: 370 |TotalExamples: 78550 |BatchLoss:   0.1143\n",
      "Epoch:   2 |Batch: 380 |TotalExamples: 79050 |BatchLoss:   0.1442\n",
      "Epoch:   2 |Batch: 390 |TotalExamples: 79550 |BatchLoss:   0.1313\n",
      "Epoch:   2 |Batch: 400 |TotalExamples: 80050 |BatchLoss:   0.1922\n",
      "Epoch:   2 |Batch: 410 |TotalExamples: 80550 |BatchLoss:   0.2087\n",
      "Epoch:   2 |Batch: 420 |TotalExamples: 81050 |BatchLoss:   0.1790\n",
      "Epoch:   2 |Batch: 430 |TotalExamples: 81550 |BatchLoss:   0.1576\n",
      "Epoch:   2 |Batch: 440 |TotalExamples: 82050 |BatchLoss:   0.1886\n",
      "Epoch:   2 |Batch: 450 |TotalExamples: 82550 |BatchLoss:   0.1691\n",
      "Epoch:   2 |Batch: 460 |TotalExamples: 83050 |BatchLoss:   0.1788\n",
      "Epoch:   2 |Batch: 470 |TotalExamples: 83550 |BatchLoss:   0.1476\n",
      "Epoch:   2 |Batch: 480 |TotalExamples: 84050 |BatchLoss:   0.2268\n",
      "Epoch:   2 |Batch: 490 |TotalExamples: 84550 |BatchLoss:   0.1518\n",
      "Epoch:   2 |Batch: 500 |TotalExamples: 85050 |BatchLoss:   0.1963\n",
      "Epoch:   2 |Batch: 510 |TotalExamples: 85550 |BatchLoss:   0.2040\n",
      "Epoch:   2 |Batch: 520 |TotalExamples: 86050 |BatchLoss:   0.1568\n",
      "Epoch:   2 |Batch: 530 |TotalExamples: 86550 |BatchLoss:   0.1813\n",
      "Epoch:   2 |Batch: 540 |TotalExamples: 87050 |BatchLoss:   0.1527\n",
      "Epoch:   2 |Batch: 550 |TotalExamples: 87550 |BatchLoss:   0.1601\n",
      "Epoch:   2 |Batch: 560 |TotalExamples: 88050 |BatchLoss:   0.2368\n",
      "Epoch:   2 |Batch: 570 |TotalExamples: 88550 |BatchLoss:   0.1915\n",
      "Epoch:   2 |Batch: 580 |TotalExamples: 89050 |BatchLoss:   0.1428\n",
      "Epoch:   2 |Batch: 590 |TotalExamples: 89550 |BatchLoss:   0.1799\n",
      "Epoch Over:   2 |MeanEpochLoss:   0.1699 |ValidationSetLoss:   0.1662 \n",
      "\n",
      "Epoch Starting: 3 \n",
      "\n",
      "Epoch:   3 |Batch:   0 |TotalExamples: 90050 |BatchLoss:   0.1186\n",
      "Epoch:   3 |Batch:  10 |TotalExamples: 90550 |BatchLoss:   0.1614\n",
      "Epoch:   3 |Batch:  20 |TotalExamples: 91050 |BatchLoss:   0.1807\n",
      "Epoch:   3 |Batch:  30 |TotalExamples: 91550 |BatchLoss:   0.1646\n",
      "Epoch:   3 |Batch:  40 |TotalExamples: 92050 |BatchLoss:   0.1637\n",
      "Epoch:   3 |Batch:  50 |TotalExamples: 92550 |BatchLoss:   0.2502\n",
      "Epoch:   3 |Batch:  60 |TotalExamples: 93050 |BatchLoss:   0.1509\n",
      "Epoch:   3 |Batch:  70 |TotalExamples: 93550 |BatchLoss:   0.1452\n",
      "Epoch:   3 |Batch:  80 |TotalExamples: 94050 |BatchLoss:   0.1442\n",
      "Epoch:   3 |Batch:  90 |TotalExamples: 94550 |BatchLoss:   0.1642\n",
      "Epoch:   3 |Batch: 100 |TotalExamples: 95050 |BatchLoss:   0.1662\n",
      "Epoch:   3 |Batch: 110 |TotalExamples: 95550 |BatchLoss:   0.1911\n",
      "Epoch:   3 |Batch: 120 |TotalExamples: 96050 |BatchLoss:   0.1520\n",
      "Epoch:   3 |Batch: 130 |TotalExamples: 96550 |BatchLoss:   0.1448\n",
      "Epoch:   3 |Batch: 140 |TotalExamples: 97050 |BatchLoss:   0.2146\n",
      "Epoch:   3 |Batch: 150 |TotalExamples: 97550 |BatchLoss:   0.1574\n",
      "Epoch:   3 |Batch: 160 |TotalExamples: 98050 |BatchLoss:   0.2277\n",
      "Epoch:   3 |Batch: 170 |TotalExamples: 98550 |BatchLoss:   0.1323\n",
      "Epoch:   3 |Batch: 180 |TotalExamples: 99050 |BatchLoss:   0.1520\n",
      "Epoch:   3 |Batch: 190 |TotalExamples: 99550 |BatchLoss:   0.1570\n",
      "Epoch:   3 |Batch: 200 |TotalExamples: 100050 |BatchLoss:   0.1129\n",
      "Epoch:   3 |Batch: 210 |TotalExamples: 100550 |BatchLoss:   0.1250\n",
      "Epoch:   3 |Batch: 220 |TotalExamples: 101050 |BatchLoss:   0.1538\n",
      "Epoch:   3 |Batch: 230 |TotalExamples: 101550 |BatchLoss:   0.1343\n",
      "Epoch:   3 |Batch: 240 |TotalExamples: 102050 |BatchLoss:   0.2192\n",
      "Epoch:   3 |Batch: 250 |TotalExamples: 102550 |BatchLoss:   0.1554\n",
      "Epoch:   3 |Batch: 260 |TotalExamples: 103050 |BatchLoss:   0.1502\n",
      "Epoch:   3 |Batch: 270 |TotalExamples: 103550 |BatchLoss:   0.1781\n",
      "Epoch:   3 |Batch: 280 |TotalExamples: 104050 |BatchLoss:   0.1213\n",
      "Epoch:   3 |Batch: 290 |TotalExamples: 104550 |BatchLoss:   0.1614\n",
      "Epoch:   3 |Batch: 300 |TotalExamples: 105050 |BatchLoss:   0.1146\n",
      "Epoch:   3 |Batch: 310 |TotalExamples: 105550 |BatchLoss:   0.2178\n",
      "Epoch:   3 |Batch: 320 |TotalExamples: 106050 |BatchLoss:   0.2086\n",
      "Epoch:   3 |Batch: 330 |TotalExamples: 106550 |BatchLoss:   0.1678\n",
      "Epoch:   3 |Batch: 340 |TotalExamples: 107050 |BatchLoss:   0.1681\n",
      "Epoch:   3 |Batch: 350 |TotalExamples: 107550 |BatchLoss:   0.1100\n",
      "Epoch:   3 |Batch: 360 |TotalExamples: 108050 |BatchLoss:   0.1806\n",
      "Epoch:   3 |Batch: 370 |TotalExamples: 108550 |BatchLoss:   0.1145\n",
      "Epoch:   3 |Batch: 380 |TotalExamples: 109050 |BatchLoss:   0.1440\n",
      "Epoch:   3 |Batch: 390 |TotalExamples: 109550 |BatchLoss:   0.1312\n",
      "Epoch:   3 |Batch: 400 |TotalExamples: 110050 |BatchLoss:   0.1931\n",
      "Epoch:   3 |Batch: 410 |TotalExamples: 110550 |BatchLoss:   0.2081\n",
      "Epoch:   3 |Batch: 420 |TotalExamples: 111050 |BatchLoss:   0.1787\n",
      "Epoch:   3 |Batch: 430 |TotalExamples: 111550 |BatchLoss:   0.1573\n",
      "Epoch:   3 |Batch: 440 |TotalExamples: 112050 |BatchLoss:   0.1891\n",
      "Epoch:   3 |Batch: 450 |TotalExamples: 112550 |BatchLoss:   0.1690\n",
      "Epoch:   3 |Batch: 460 |TotalExamples: 113050 |BatchLoss:   0.1793\n",
      "Epoch:   3 |Batch: 470 |TotalExamples: 113550 |BatchLoss:   0.1466\n",
      "Epoch:   3 |Batch: 480 |TotalExamples: 114050 |BatchLoss:   0.2297\n",
      "Epoch:   3 |Batch: 490 |TotalExamples: 114550 |BatchLoss:   0.1523\n",
      "Epoch:   3 |Batch: 500 |TotalExamples: 115050 |BatchLoss:   0.1956\n",
      "Epoch:   3 |Batch: 510 |TotalExamples: 115550 |BatchLoss:   0.2039\n",
      "Epoch:   3 |Batch: 520 |TotalExamples: 116050 |BatchLoss:   0.1567\n",
      "Epoch:   3 |Batch: 530 |TotalExamples: 116550 |BatchLoss:   0.1814\n",
      "Epoch:   3 |Batch: 540 |TotalExamples: 117050 |BatchLoss:   0.1533\n",
      "Epoch:   3 |Batch: 550 |TotalExamples: 117550 |BatchLoss:   0.1582\n",
      "Epoch:   3 |Batch: 560 |TotalExamples: 118050 |BatchLoss:   0.2367\n",
      "Epoch:   3 |Batch: 570 |TotalExamples: 118550 |BatchLoss:   0.1905\n",
      "Epoch:   3 |Batch: 580 |TotalExamples: 119050 |BatchLoss:   0.1432\n",
      "Epoch:   3 |Batch: 590 |TotalExamples: 119550 |BatchLoss:   0.1805\n",
      "Epoch Over:   3 |MeanEpochLoss:   0.1696 |ValidationSetLoss:   0.1662 \n",
      "\n",
      "Epoch Starting: 4 \n",
      "\n",
      "Epoch:   4 |Batch:   0 |TotalExamples: 120050 |BatchLoss:   0.1188\n",
      "Epoch:   4 |Batch:  10 |TotalExamples: 120550 |BatchLoss:   0.1626\n",
      "Epoch:   4 |Batch:  20 |TotalExamples: 121050 |BatchLoss:   0.1813\n",
      "Epoch:   4 |Batch:  30 |TotalExamples: 121550 |BatchLoss:   0.1635\n",
      "Epoch:   4 |Batch:  40 |TotalExamples: 122050 |BatchLoss:   0.1631\n",
      "Epoch:   4 |Batch:  50 |TotalExamples: 122550 |BatchLoss:   0.2500\n",
      "Epoch:   4 |Batch:  60 |TotalExamples: 123050 |BatchLoss:   0.1510\n",
      "Epoch:   4 |Batch:  70 |TotalExamples: 123550 |BatchLoss:   0.1458\n",
      "Epoch:   4 |Batch:  80 |TotalExamples: 124050 |BatchLoss:   0.1441\n",
      "Epoch:   4 |Batch:  90 |TotalExamples: 124550 |BatchLoss:   0.1638\n",
      "Epoch:   4 |Batch: 100 |TotalExamples: 125050 |BatchLoss:   0.1659\n",
      "Epoch:   4 |Batch: 110 |TotalExamples: 125550 |BatchLoss:   0.1913\n",
      "Epoch:   4 |Batch: 120 |TotalExamples: 126050 |BatchLoss:   0.1525\n",
      "Epoch:   4 |Batch: 130 |TotalExamples: 126550 |BatchLoss:   0.1443\n",
      "Epoch:   4 |Batch: 140 |TotalExamples: 127050 |BatchLoss:   0.2140\n",
      "Epoch:   4 |Batch: 150 |TotalExamples: 127550 |BatchLoss:   0.1576\n",
      "Epoch:   4 |Batch: 160 |TotalExamples: 128050 |BatchLoss:   0.2275\n",
      "Epoch:   4 |Batch: 170 |TotalExamples: 128550 |BatchLoss:   0.1324\n",
      "Epoch:   4 |Batch: 180 |TotalExamples: 129050 |BatchLoss:   0.1521\n",
      "Epoch:   4 |Batch: 190 |TotalExamples: 129550 |BatchLoss:   0.1559\n",
      "Epoch:   4 |Batch: 200 |TotalExamples: 130050 |BatchLoss:   0.1118\n",
      "Epoch:   4 |Batch: 210 |TotalExamples: 130550 |BatchLoss:   0.1251\n",
      "Epoch:   4 |Batch: 220 |TotalExamples: 131050 |BatchLoss:   0.1535\n",
      "Epoch:   4 |Batch: 230 |TotalExamples: 131550 |BatchLoss:   0.1339\n",
      "Epoch:   4 |Batch: 240 |TotalExamples: 132050 |BatchLoss:   0.2182\n",
      "Epoch:   4 |Batch: 250 |TotalExamples: 132550 |BatchLoss:   0.1550\n",
      "Epoch:   4 |Batch: 260 |TotalExamples: 133050 |BatchLoss:   0.1502\n",
      "Epoch:   4 |Batch: 270 |TotalExamples: 133550 |BatchLoss:   0.1779\n",
      "Epoch:   4 |Batch: 280 |TotalExamples: 134050 |BatchLoss:   0.1212\n",
      "Epoch:   4 |Batch: 290 |TotalExamples: 134550 |BatchLoss:   0.1610\n",
      "Epoch:   4 |Batch: 300 |TotalExamples: 135050 |BatchLoss:   0.1148\n",
      "Epoch:   4 |Batch: 310 |TotalExamples: 135550 |BatchLoss:   0.2176\n",
      "Epoch:   4 |Batch: 320 |TotalExamples: 136050 |BatchLoss:   0.2085\n",
      "Epoch:   4 |Batch: 330 |TotalExamples: 136550 |BatchLoss:   0.1680\n",
      "Epoch:   4 |Batch: 340 |TotalExamples: 137050 |BatchLoss:   0.1676\n",
      "Epoch:   4 |Batch: 350 |TotalExamples: 137550 |BatchLoss:   0.1100\n",
      "Epoch:   4 |Batch: 360 |TotalExamples: 138050 |BatchLoss:   0.1806\n",
      "Epoch:   4 |Batch: 370 |TotalExamples: 138550 |BatchLoss:   0.1145\n",
      "Epoch:   4 |Batch: 380 |TotalExamples: 139050 |BatchLoss:   0.1427\n",
      "Epoch:   4 |Batch: 390 |TotalExamples: 139550 |BatchLoss:   0.1311\n",
      "Epoch:   4 |Batch: 400 |TotalExamples: 140050 |BatchLoss:   0.1935\n",
      "Epoch:   4 |Batch: 410 |TotalExamples: 140550 |BatchLoss:   0.2077\n",
      "Epoch:   4 |Batch: 420 |TotalExamples: 141050 |BatchLoss:   0.1791\n",
      "Epoch:   4 |Batch: 430 |TotalExamples: 141550 |BatchLoss:   0.1572\n",
      "Epoch:   4 |Batch: 440 |TotalExamples: 142050 |BatchLoss:   0.1903\n",
      "Epoch:   4 |Batch: 450 |TotalExamples: 142550 |BatchLoss:   0.1692\n",
      "Epoch:   4 |Batch: 460 |TotalExamples: 143050 |BatchLoss:   0.1793\n",
      "Epoch:   4 |Batch: 470 |TotalExamples: 143550 |BatchLoss:   0.1467\n",
      "Epoch:   4 |Batch: 480 |TotalExamples: 144050 |BatchLoss:   0.2305\n",
      "Epoch:   4 |Batch: 490 |TotalExamples: 144550 |BatchLoss:   0.1527\n",
      "Epoch:   4 |Batch: 500 |TotalExamples: 145050 |BatchLoss:   0.1952\n",
      "Epoch:   4 |Batch: 510 |TotalExamples: 145550 |BatchLoss:   0.2028\n",
      "Epoch:   4 |Batch: 520 |TotalExamples: 146050 |BatchLoss:   0.1567\n",
      "Epoch:   4 |Batch: 530 |TotalExamples: 146550 |BatchLoss:   0.1814\n",
      "Epoch:   4 |Batch: 540 |TotalExamples: 147050 |BatchLoss:   0.1533\n",
      "Epoch:   4 |Batch: 550 |TotalExamples: 147550 |BatchLoss:   0.1579\n",
      "Epoch:   4 |Batch: 560 |TotalExamples: 148050 |BatchLoss:   0.2369\n",
      "Epoch:   4 |Batch: 570 |TotalExamples: 148550 |BatchLoss:   0.1906\n",
      "Epoch:   4 |Batch: 580 |TotalExamples: 149050 |BatchLoss:   0.1433\n",
      "Epoch:   4 |Batch: 590 |TotalExamples: 149550 |BatchLoss:   0.1813\n",
      "Epoch Over:   4 |MeanEpochLoss:   0.1694 |ValidationSetLoss:   0.1664 \n",
      "\n",
      "Epoch Starting: 5 \n",
      "\n",
      "Epoch:   5 |Batch:   0 |TotalExamples: 150050 |BatchLoss:   0.1191\n",
      "Epoch:   5 |Batch:  10 |TotalExamples: 150550 |BatchLoss:   0.1634\n",
      "Epoch:   5 |Batch:  20 |TotalExamples: 151050 |BatchLoss:   0.1820\n",
      "Epoch:   5 |Batch:  30 |TotalExamples: 151550 |BatchLoss:   0.1631\n",
      "Epoch:   5 |Batch:  40 |TotalExamples: 152050 |BatchLoss:   0.1626\n",
      "Epoch:   5 |Batch:  50 |TotalExamples: 152550 |BatchLoss:   0.2487\n",
      "Epoch:   5 |Batch:  60 |TotalExamples: 153050 |BatchLoss:   0.1509\n",
      "Epoch:   5 |Batch:  70 |TotalExamples: 153550 |BatchLoss:   0.1459\n",
      "Epoch:   5 |Batch:  80 |TotalExamples: 154050 |BatchLoss:   0.1442\n",
      "Epoch:   5 |Batch:  90 |TotalExamples: 154550 |BatchLoss:   0.1633\n",
      "Epoch:   5 |Batch: 100 |TotalExamples: 155050 |BatchLoss:   0.1650\n",
      "Epoch:   5 |Batch: 110 |TotalExamples: 155550 |BatchLoss:   0.1914\n",
      "Epoch:   5 |Batch: 120 |TotalExamples: 156050 |BatchLoss:   0.1527\n",
      "Epoch:   5 |Batch: 130 |TotalExamples: 156550 |BatchLoss:   0.1438\n",
      "Epoch:   5 |Batch: 140 |TotalExamples: 157050 |BatchLoss:   0.2138\n",
      "Epoch:   5 |Batch: 150 |TotalExamples: 157550 |BatchLoss:   0.1576\n",
      "Epoch:   5 |Batch: 160 |TotalExamples: 158050 |BatchLoss:   0.2278\n",
      "Epoch:   5 |Batch: 170 |TotalExamples: 158550 |BatchLoss:   0.1328\n",
      "Epoch:   5 |Batch: 180 |TotalExamples: 159050 |BatchLoss:   0.1520\n",
      "Epoch:   5 |Batch: 190 |TotalExamples: 159550 |BatchLoss:   0.1552\n",
      "Epoch:   5 |Batch: 200 |TotalExamples: 160050 |BatchLoss:   0.1113\n",
      "Epoch:   5 |Batch: 210 |TotalExamples: 160550 |BatchLoss:   0.1256\n",
      "Epoch:   5 |Batch: 220 |TotalExamples: 161050 |BatchLoss:   0.1530\n",
      "Epoch:   5 |Batch: 230 |TotalExamples: 161550 |BatchLoss:   0.1335\n",
      "Epoch:   5 |Batch: 240 |TotalExamples: 162050 |BatchLoss:   0.2177\n",
      "Epoch:   5 |Batch: 250 |TotalExamples: 162550 |BatchLoss:   0.1545\n",
      "Epoch:   5 |Batch: 260 |TotalExamples: 163050 |BatchLoss:   0.1504\n",
      "Epoch:   5 |Batch: 270 |TotalExamples: 163550 |BatchLoss:   0.1774\n",
      "Epoch:   5 |Batch: 280 |TotalExamples: 164050 |BatchLoss:   0.1205\n",
      "Epoch:   5 |Batch: 290 |TotalExamples: 164550 |BatchLoss:   0.1615\n",
      "Epoch:   5 |Batch: 300 |TotalExamples: 165050 |BatchLoss:   0.1144\n",
      "Epoch:   5 |Batch: 310 |TotalExamples: 165550 |BatchLoss:   0.2173\n",
      "Epoch:   5 |Batch: 320 |TotalExamples: 166050 |BatchLoss:   0.2090\n",
      "Epoch:   5 |Batch: 330 |TotalExamples: 166550 |BatchLoss:   0.1680\n",
      "Epoch:   5 |Batch: 340 |TotalExamples: 167050 |BatchLoss:   0.1671\n",
      "Epoch:   5 |Batch: 350 |TotalExamples: 167550 |BatchLoss:   0.1100\n",
      "Epoch:   5 |Batch: 360 |TotalExamples: 168050 |BatchLoss:   0.1807\n",
      "Epoch:   5 |Batch: 370 |TotalExamples: 168550 |BatchLoss:   0.1146\n",
      "Epoch:   5 |Batch: 380 |TotalExamples: 169050 |BatchLoss:   0.1429\n",
      "Epoch:   5 |Batch: 390 |TotalExamples: 169550 |BatchLoss:   0.1316\n",
      "Epoch:   5 |Batch: 400 |TotalExamples: 170050 |BatchLoss:   0.1939\n",
      "Epoch:   5 |Batch: 410 |TotalExamples: 170550 |BatchLoss:   0.2069\n",
      "Epoch:   5 |Batch: 420 |TotalExamples: 171050 |BatchLoss:   0.1790\n",
      "Epoch:   5 |Batch: 430 |TotalExamples: 171550 |BatchLoss:   0.1574\n",
      "Epoch:   5 |Batch: 440 |TotalExamples: 172050 |BatchLoss:   0.1908\n",
      "Epoch:   5 |Batch: 450 |TotalExamples: 172550 |BatchLoss:   0.1691\n",
      "Epoch:   5 |Batch: 460 |TotalExamples: 173050 |BatchLoss:   0.1791\n",
      "Epoch:   5 |Batch: 470 |TotalExamples: 173550 |BatchLoss:   0.1468\n",
      "Epoch:   5 |Batch: 480 |TotalExamples: 174050 |BatchLoss:   0.2316\n",
      "Epoch:   5 |Batch: 490 |TotalExamples: 174550 |BatchLoss:   0.1529\n",
      "Epoch:   5 |Batch: 500 |TotalExamples: 175050 |BatchLoss:   0.1953\n",
      "Epoch:   5 |Batch: 510 |TotalExamples: 175550 |BatchLoss:   0.2037\n",
      "Epoch:   5 |Batch: 520 |TotalExamples: 176050 |BatchLoss:   0.1570\n",
      "Epoch:   5 |Batch: 530 |TotalExamples: 176550 |BatchLoss:   0.1819\n",
      "Epoch:   5 |Batch: 540 |TotalExamples: 177050 |BatchLoss:   0.1538\n",
      "Epoch:   5 |Batch: 550 |TotalExamples: 177550 |BatchLoss:   0.1569\n",
      "Epoch:   5 |Batch: 560 |TotalExamples: 178050 |BatchLoss:   0.2369\n",
      "Epoch:   5 |Batch: 570 |TotalExamples: 178550 |BatchLoss:   0.1889\n",
      "Epoch:   5 |Batch: 580 |TotalExamples: 179050 |BatchLoss:   0.1430\n",
      "Epoch:   5 |Batch: 590 |TotalExamples: 179550 |BatchLoss:   0.1813\n",
      "Epoch Over:   5 |MeanEpochLoss:   0.1693 |ValidationSetLoss:   0.1665 \n",
      "\n",
      "Epoch Starting: 6 \n",
      "\n",
      "Epoch:   6 |Batch:   0 |TotalExamples: 180050 |BatchLoss:   0.1195\n",
      "Epoch:   6 |Batch:  10 |TotalExamples: 180550 |BatchLoss:   0.1639\n",
      "Epoch:   6 |Batch:  20 |TotalExamples: 181050 |BatchLoss:   0.1823\n",
      "Epoch:   6 |Batch:  30 |TotalExamples: 181550 |BatchLoss:   0.1628\n",
      "Epoch:   6 |Batch:  40 |TotalExamples: 182050 |BatchLoss:   0.1618\n",
      "Epoch:   6 |Batch:  50 |TotalExamples: 182550 |BatchLoss:   0.2487\n",
      "Epoch:   6 |Batch:  60 |TotalExamples: 183050 |BatchLoss:   0.1514\n",
      "Epoch:   6 |Batch:  70 |TotalExamples: 183550 |BatchLoss:   0.1461\n",
      "Epoch:   6 |Batch:  80 |TotalExamples: 184050 |BatchLoss:   0.1446\n",
      "Epoch:   6 |Batch:  90 |TotalExamples: 184550 |BatchLoss:   0.1640\n",
      "Epoch:   6 |Batch: 100 |TotalExamples: 185050 |BatchLoss:   0.1654\n",
      "Epoch:   6 |Batch: 110 |TotalExamples: 185550 |BatchLoss:   0.1908\n",
      "Epoch:   6 |Batch: 120 |TotalExamples: 186050 |BatchLoss:   0.1525\n",
      "Epoch:   6 |Batch: 130 |TotalExamples: 186550 |BatchLoss:   0.1436\n",
      "Epoch:   6 |Batch: 140 |TotalExamples: 187050 |BatchLoss:   0.2135\n",
      "Epoch:   6 |Batch: 150 |TotalExamples: 187550 |BatchLoss:   0.1581\n",
      "Epoch:   6 |Batch: 160 |TotalExamples: 188050 |BatchLoss:   0.2275\n",
      "Epoch:   6 |Batch: 170 |TotalExamples: 188550 |BatchLoss:   0.1329\n",
      "Epoch:   6 |Batch: 180 |TotalExamples: 189050 |BatchLoss:   0.1524\n",
      "Epoch:   6 |Batch: 190 |TotalExamples: 189550 |BatchLoss:   0.1547\n",
      "Epoch:   6 |Batch: 200 |TotalExamples: 190050 |BatchLoss:   0.1102\n",
      "Epoch:   6 |Batch: 210 |TotalExamples: 190550 |BatchLoss:   0.1264\n",
      "Epoch:   6 |Batch: 220 |TotalExamples: 191050 |BatchLoss:   0.1528\n",
      "Epoch:   6 |Batch: 230 |TotalExamples: 191550 |BatchLoss:   0.1333\n",
      "Epoch:   6 |Batch: 240 |TotalExamples: 192050 |BatchLoss:   0.2177\n",
      "Epoch:   6 |Batch: 250 |TotalExamples: 192550 |BatchLoss:   0.1546\n",
      "Epoch:   6 |Batch: 260 |TotalExamples: 193050 |BatchLoss:   0.1507\n",
      "Epoch:   6 |Batch: 270 |TotalExamples: 193550 |BatchLoss:   0.1771\n",
      "Epoch:   6 |Batch: 280 |TotalExamples: 194050 |BatchLoss:   0.1203\n",
      "Epoch:   6 |Batch: 290 |TotalExamples: 194550 |BatchLoss:   0.1616\n",
      "Epoch:   6 |Batch: 300 |TotalExamples: 195050 |BatchLoss:   0.1144\n",
      "Epoch:   6 |Batch: 310 |TotalExamples: 195550 |BatchLoss:   0.2163\n",
      "Epoch:   6 |Batch: 320 |TotalExamples: 196050 |BatchLoss:   0.2088\n",
      "Epoch:   6 |Batch: 330 |TotalExamples: 196550 |BatchLoss:   0.1677\n",
      "Epoch:   6 |Batch: 340 |TotalExamples: 197050 |BatchLoss:   0.1669\n",
      "Epoch:   6 |Batch: 350 |TotalExamples: 197550 |BatchLoss:   0.1098\n",
      "Epoch:   6 |Batch: 360 |TotalExamples: 198050 |BatchLoss:   0.1807\n",
      "Epoch:   6 |Batch: 370 |TotalExamples: 198550 |BatchLoss:   0.1148\n",
      "Epoch:   6 |Batch: 380 |TotalExamples: 199050 |BatchLoss:   0.1432\n",
      "Epoch:   6 |Batch: 390 |TotalExamples: 199550 |BatchLoss:   0.1315\n",
      "Epoch:   6 |Batch: 400 |TotalExamples: 200050 |BatchLoss:   0.1943\n",
      "Epoch:   6 |Batch: 410 |TotalExamples: 200550 |BatchLoss:   0.2060\n",
      "Epoch:   6 |Batch: 420 |TotalExamples: 201050 |BatchLoss:   0.1792\n",
      "Epoch:   6 |Batch: 430 |TotalExamples: 201550 |BatchLoss:   0.1579\n",
      "Epoch:   6 |Batch: 440 |TotalExamples: 202050 |BatchLoss:   0.1915\n",
      "Epoch:   6 |Batch: 450 |TotalExamples: 202550 |BatchLoss:   0.1694\n",
      "Epoch:   6 |Batch: 460 |TotalExamples: 203050 |BatchLoss:   0.1786\n",
      "Epoch:   6 |Batch: 470 |TotalExamples: 203550 |BatchLoss:   0.1471\n",
      "Epoch:   6 |Batch: 480 |TotalExamples: 204050 |BatchLoss:   0.2318\n",
      "Epoch:   6 |Batch: 490 |TotalExamples: 204550 |BatchLoss:   0.1531\n",
      "Epoch:   6 |Batch: 500 |TotalExamples: 205050 |BatchLoss:   0.1952\n",
      "Epoch:   6 |Batch: 510 |TotalExamples: 205550 |BatchLoss:   0.2035\n",
      "Epoch:   6 |Batch: 520 |TotalExamples: 206050 |BatchLoss:   0.1567\n",
      "Epoch:   6 |Batch: 530 |TotalExamples: 206550 |BatchLoss:   0.1821\n",
      "Epoch:   6 |Batch: 540 |TotalExamples: 207050 |BatchLoss:   0.1540\n",
      "Epoch:   6 |Batch: 550 |TotalExamples: 207550 |BatchLoss:   0.1565\n",
      "Epoch:   6 |Batch: 560 |TotalExamples: 208050 |BatchLoss:   0.2373\n",
      "Epoch:   6 |Batch: 570 |TotalExamples: 208550 |BatchLoss:   0.1888\n",
      "Epoch:   6 |Batch: 580 |TotalExamples: 209050 |BatchLoss:   0.1432\n",
      "Epoch:   6 |Batch: 590 |TotalExamples: 209550 |BatchLoss:   0.1813\n",
      "Epoch Over:   6 |MeanEpochLoss:   0.1692 |ValidationSetLoss:   0.1667 \n",
      "\n",
      "Epoch Starting: 7 \n",
      "\n",
      "Epoch:   7 |Batch:   0 |TotalExamples: 210050 |BatchLoss:   0.1199\n",
      "Epoch:   7 |Batch:  10 |TotalExamples: 210550 |BatchLoss:   0.1638\n",
      "Epoch:   7 |Batch:  20 |TotalExamples: 211050 |BatchLoss:   0.1827\n",
      "Epoch:   7 |Batch:  30 |TotalExamples: 211550 |BatchLoss:   0.1623\n",
      "Epoch:   7 |Batch:  40 |TotalExamples: 212050 |BatchLoss:   0.1614\n",
      "Epoch:   7 |Batch:  50 |TotalExamples: 212550 |BatchLoss:   0.2482\n",
      "Epoch:   7 |Batch:  60 |TotalExamples: 213050 |BatchLoss:   0.1516\n",
      "Epoch:   7 |Batch:  70 |TotalExamples: 213550 |BatchLoss:   0.1459\n",
      "Epoch:   7 |Batch:  80 |TotalExamples: 214050 |BatchLoss:   0.1449\n",
      "Epoch:   7 |Batch:  90 |TotalExamples: 214550 |BatchLoss:   0.1635\n",
      "Epoch:   7 |Batch: 100 |TotalExamples: 215050 |BatchLoss:   0.1645\n",
      "Epoch:   7 |Batch: 110 |TotalExamples: 215550 |BatchLoss:   0.1912\n",
      "Epoch:   7 |Batch: 120 |TotalExamples: 216050 |BatchLoss:   0.1528\n",
      "Epoch:   7 |Batch: 130 |TotalExamples: 216550 |BatchLoss:   0.1434\n",
      "Epoch:   7 |Batch: 140 |TotalExamples: 217050 |BatchLoss:   0.2131\n",
      "Epoch:   7 |Batch: 150 |TotalExamples: 217550 |BatchLoss:   0.1578\n",
      "Epoch:   7 |Batch: 160 |TotalExamples: 218050 |BatchLoss:   0.2279\n",
      "Epoch:   7 |Batch: 170 |TotalExamples: 218550 |BatchLoss:   0.1333\n",
      "Epoch:   7 |Batch: 180 |TotalExamples: 219050 |BatchLoss:   0.1523\n",
      "Epoch:   7 |Batch: 190 |TotalExamples: 219550 |BatchLoss:   0.1535\n",
      "Epoch:   7 |Batch: 200 |TotalExamples: 220050 |BatchLoss:   0.1092\n",
      "Epoch:   7 |Batch: 210 |TotalExamples: 220550 |BatchLoss:   0.1260\n",
      "Epoch:   7 |Batch: 220 |TotalExamples: 221050 |BatchLoss:   0.1525\n",
      "Epoch:   7 |Batch: 230 |TotalExamples: 221550 |BatchLoss:   0.1333\n",
      "Epoch:   7 |Batch: 240 |TotalExamples: 222050 |BatchLoss:   0.2174\n",
      "Epoch:   7 |Batch: 250 |TotalExamples: 222550 |BatchLoss:   0.1544\n",
      "Epoch:   7 |Batch: 260 |TotalExamples: 223050 |BatchLoss:   0.1508\n",
      "Epoch:   7 |Batch: 270 |TotalExamples: 223550 |BatchLoss:   0.1768\n",
      "Epoch:   7 |Batch: 280 |TotalExamples: 224050 |BatchLoss:   0.1198\n",
      "Epoch:   7 |Batch: 290 |TotalExamples: 224550 |BatchLoss:   0.1615\n",
      "Epoch:   7 |Batch: 300 |TotalExamples: 225050 |BatchLoss:   0.1149\n",
      "Epoch:   7 |Batch: 310 |TotalExamples: 225550 |BatchLoss:   0.2158\n",
      "Epoch:   7 |Batch: 320 |TotalExamples: 226050 |BatchLoss:   0.2086\n",
      "Epoch:   7 |Batch: 330 |TotalExamples: 226550 |BatchLoss:   0.1675\n",
      "Epoch:   7 |Batch: 340 |TotalExamples: 227050 |BatchLoss:   0.1668\n",
      "Epoch:   7 |Batch: 350 |TotalExamples: 227550 |BatchLoss:   0.1098\n",
      "Epoch:   7 |Batch: 360 |TotalExamples: 228050 |BatchLoss:   0.1804\n",
      "Epoch:   7 |Batch: 370 |TotalExamples: 228550 |BatchLoss:   0.1152\n",
      "Epoch:   7 |Batch: 380 |TotalExamples: 229050 |BatchLoss:   0.1437\n",
      "Epoch:   7 |Batch: 390 |TotalExamples: 229550 |BatchLoss:   0.1318\n",
      "Epoch:   7 |Batch: 400 |TotalExamples: 230050 |BatchLoss:   0.1946\n",
      "Epoch:   7 |Batch: 410 |TotalExamples: 230550 |BatchLoss:   0.2058\n",
      "Epoch:   7 |Batch: 420 |TotalExamples: 231050 |BatchLoss:   0.1793\n",
      "Epoch:   7 |Batch: 430 |TotalExamples: 231550 |BatchLoss:   0.1574\n",
      "Epoch:   7 |Batch: 440 |TotalExamples: 232050 |BatchLoss:   0.1921\n",
      "Epoch:   7 |Batch: 450 |TotalExamples: 232550 |BatchLoss:   0.1697\n",
      "Epoch:   7 |Batch: 460 |TotalExamples: 233050 |BatchLoss:   0.1784\n",
      "Epoch:   7 |Batch: 470 |TotalExamples: 233550 |BatchLoss:   0.1470\n",
      "Epoch:   7 |Batch: 480 |TotalExamples: 234050 |BatchLoss:   0.2336\n",
      "Epoch:   7 |Batch: 490 |TotalExamples: 234550 |BatchLoss:   0.1529\n",
      "Epoch:   7 |Batch: 500 |TotalExamples: 235050 |BatchLoss:   0.1952\n",
      "Epoch:   7 |Batch: 510 |TotalExamples: 235550 |BatchLoss:   0.2040\n",
      "Epoch:   7 |Batch: 520 |TotalExamples: 236050 |BatchLoss:   0.1570\n",
      "Epoch:   7 |Batch: 530 |TotalExamples: 236550 |BatchLoss:   0.1830\n",
      "Epoch:   7 |Batch: 540 |TotalExamples: 237050 |BatchLoss:   0.1539\n",
      "Epoch:   7 |Batch: 550 |TotalExamples: 237550 |BatchLoss:   0.1563\n",
      "Epoch:   7 |Batch: 560 |TotalExamples: 238050 |BatchLoss:   0.2370\n",
      "Epoch:   7 |Batch: 570 |TotalExamples: 238550 |BatchLoss:   0.1897\n",
      "Epoch:   7 |Batch: 580 |TotalExamples: 239050 |BatchLoss:   0.1432\n",
      "Epoch:   7 |Batch: 590 |TotalExamples: 239550 |BatchLoss:   0.1814\n",
      "Epoch Over:   7 |MeanEpochLoss:   0.1691 |ValidationSetLoss:   0.1669 \n",
      "\n",
      "Epoch Starting: 8 \n",
      "\n",
      "Epoch:   8 |Batch:   0 |TotalExamples: 240050 |BatchLoss:   0.1199\n",
      "Epoch:   8 |Batch:  10 |TotalExamples: 240550 |BatchLoss:   0.1643\n",
      "Epoch:   8 |Batch:  20 |TotalExamples: 241050 |BatchLoss:   0.1833\n",
      "Epoch:   8 |Batch:  30 |TotalExamples: 241550 |BatchLoss:   0.1619\n",
      "Epoch:   8 |Batch:  40 |TotalExamples: 242050 |BatchLoss:   0.1617\n",
      "Epoch:   8 |Batch:  50 |TotalExamples: 242550 |BatchLoss:   0.2483\n",
      "Epoch:   8 |Batch:  60 |TotalExamples: 243050 |BatchLoss:   0.1518\n",
      "Epoch:   8 |Batch:  70 |TotalExamples: 243550 |BatchLoss:   0.1455\n",
      "Epoch:   8 |Batch:  80 |TotalExamples: 244050 |BatchLoss:   0.1456\n",
      "Epoch:   8 |Batch:  90 |TotalExamples: 244550 |BatchLoss:   0.1634\n",
      "Epoch:   8 |Batch: 100 |TotalExamples: 245050 |BatchLoss:   0.1648\n",
      "Epoch:   8 |Batch: 110 |TotalExamples: 245550 |BatchLoss:   0.1908\n",
      "Epoch:   8 |Batch: 120 |TotalExamples: 246050 |BatchLoss:   0.1527\n",
      "Epoch:   8 |Batch: 130 |TotalExamples: 246550 |BatchLoss:   0.1431\n",
      "Epoch:   8 |Batch: 140 |TotalExamples: 247050 |BatchLoss:   0.2135\n",
      "Epoch:   8 |Batch: 150 |TotalExamples: 247550 |BatchLoss:   0.1578\n",
      "Epoch:   8 |Batch: 160 |TotalExamples: 248050 |BatchLoss:   0.2276\n",
      "Epoch:   8 |Batch: 170 |TotalExamples: 248550 |BatchLoss:   0.1334\n",
      "Epoch:   8 |Batch: 180 |TotalExamples: 249050 |BatchLoss:   0.1520\n",
      "Epoch:   8 |Batch: 190 |TotalExamples: 249550 |BatchLoss:   0.1534\n",
      "Epoch:   8 |Batch: 200 |TotalExamples: 250050 |BatchLoss:   0.1105\n",
      "Epoch:   8 |Batch: 210 |TotalExamples: 250550 |BatchLoss:   0.1265\n",
      "Epoch:   8 |Batch: 220 |TotalExamples: 251050 |BatchLoss:   0.1520\n",
      "Epoch:   8 |Batch: 230 |TotalExamples: 251550 |BatchLoss:   0.1333\n",
      "Epoch:   8 |Batch: 240 |TotalExamples: 252050 |BatchLoss:   0.2168\n",
      "Epoch:   8 |Batch: 250 |TotalExamples: 252550 |BatchLoss:   0.1540\n",
      "Epoch:   8 |Batch: 260 |TotalExamples: 253050 |BatchLoss:   0.1510\n",
      "Epoch:   8 |Batch: 270 |TotalExamples: 253550 |BatchLoss:   0.1768\n",
      "Epoch:   8 |Batch: 280 |TotalExamples: 254050 |BatchLoss:   0.1196\n",
      "Epoch:   8 |Batch: 290 |TotalExamples: 254550 |BatchLoss:   0.1618\n",
      "Epoch:   8 |Batch: 300 |TotalExamples: 255050 |BatchLoss:   0.1148\n",
      "Epoch:   8 |Batch: 310 |TotalExamples: 255550 |BatchLoss:   0.2157\n",
      "Epoch:   8 |Batch: 320 |TotalExamples: 256050 |BatchLoss:   0.2088\n",
      "Epoch:   8 |Batch: 330 |TotalExamples: 256550 |BatchLoss:   0.1670\n",
      "Epoch:   8 |Batch: 340 |TotalExamples: 257050 |BatchLoss:   0.1666\n",
      "Epoch:   8 |Batch: 350 |TotalExamples: 257550 |BatchLoss:   0.1098\n",
      "Epoch:   8 |Batch: 360 |TotalExamples: 258050 |BatchLoss:   0.1808\n",
      "Epoch:   8 |Batch: 370 |TotalExamples: 258550 |BatchLoss:   0.1157\n",
      "Epoch:   8 |Batch: 380 |TotalExamples: 259050 |BatchLoss:   0.1432\n",
      "Epoch:   8 |Batch: 390 |TotalExamples: 259550 |BatchLoss:   0.1316\n",
      "Epoch:   8 |Batch: 400 |TotalExamples: 260050 |BatchLoss:   0.1944\n",
      "Epoch:   8 |Batch: 410 |TotalExamples: 260550 |BatchLoss:   0.2052\n",
      "Epoch:   8 |Batch: 420 |TotalExamples: 261050 |BatchLoss:   0.1790\n",
      "Epoch:   8 |Batch: 430 |TotalExamples: 261550 |BatchLoss:   0.1575\n",
      "Epoch:   8 |Batch: 440 |TotalExamples: 262050 |BatchLoss:   0.1926\n",
      "Epoch:   8 |Batch: 450 |TotalExamples: 262550 |BatchLoss:   0.1695\n",
      "Epoch:   8 |Batch: 460 |TotalExamples: 263050 |BatchLoss:   0.1783\n",
      "Epoch:   8 |Batch: 470 |TotalExamples: 263550 |BatchLoss:   0.1473\n",
      "Epoch:   8 |Batch: 480 |TotalExamples: 264050 |BatchLoss:   0.2337\n",
      "Epoch:   8 |Batch: 490 |TotalExamples: 264550 |BatchLoss:   0.1532\n",
      "Epoch:   8 |Batch: 500 |TotalExamples: 265050 |BatchLoss:   0.1954\n",
      "Epoch:   8 |Batch: 510 |TotalExamples: 265550 |BatchLoss:   0.2042\n",
      "Epoch:   8 |Batch: 520 |TotalExamples: 266050 |BatchLoss:   0.1576\n",
      "Epoch:   8 |Batch: 530 |TotalExamples: 266550 |BatchLoss:   0.1829\n",
      "Epoch:   8 |Batch: 540 |TotalExamples: 267050 |BatchLoss:   0.1540\n",
      "Epoch:   8 |Batch: 550 |TotalExamples: 267550 |BatchLoss:   0.1558\n",
      "Epoch:   8 |Batch: 560 |TotalExamples: 268050 |BatchLoss:   0.2364\n",
      "Epoch:   8 |Batch: 570 |TotalExamples: 268550 |BatchLoss:   0.1885\n",
      "Epoch:   8 |Batch: 580 |TotalExamples: 269050 |BatchLoss:   0.1438\n",
      "Epoch:   8 |Batch: 590 |TotalExamples: 269550 |BatchLoss:   0.1817\n",
      "Epoch Over:   8 |MeanEpochLoss:   0.1690 |ValidationSetLoss:   0.1669 \n",
      "\n",
      "Epoch Starting: 9 \n",
      "\n",
      "Epoch:   9 |Batch:   0 |TotalExamples: 270050 |BatchLoss:   0.1197\n",
      "Epoch:   9 |Batch:  10 |TotalExamples: 270550 |BatchLoss:   0.1645\n",
      "Epoch:   9 |Batch:  20 |TotalExamples: 271050 |BatchLoss:   0.1838\n",
      "Epoch:   9 |Batch:  30 |TotalExamples: 271550 |BatchLoss:   0.1617\n",
      "Epoch:   9 |Batch:  40 |TotalExamples: 272050 |BatchLoss:   0.1615\n",
      "Epoch:   9 |Batch:  50 |TotalExamples: 272550 |BatchLoss:   0.2477\n",
      "Epoch:   9 |Batch:  60 |TotalExamples: 273050 |BatchLoss:   0.1522\n",
      "Epoch:   9 |Batch:  70 |TotalExamples: 273550 |BatchLoss:   0.1458\n",
      "Epoch:   9 |Batch:  80 |TotalExamples: 274050 |BatchLoss:   0.1440\n",
      "Epoch:   9 |Batch:  90 |TotalExamples: 274550 |BatchLoss:   0.1631\n",
      "Epoch:   9 |Batch: 100 |TotalExamples: 275050 |BatchLoss:   0.1648\n",
      "Epoch:   9 |Batch: 110 |TotalExamples: 275550 |BatchLoss:   0.1902\n",
      "Epoch:   9 |Batch: 120 |TotalExamples: 276050 |BatchLoss:   0.1530\n",
      "Epoch:   9 |Batch: 130 |TotalExamples: 276550 |BatchLoss:   0.1431\n",
      "Epoch:   9 |Batch: 140 |TotalExamples: 277050 |BatchLoss:   0.2133\n",
      "Epoch:   9 |Batch: 150 |TotalExamples: 277550 |BatchLoss:   0.1573\n",
      "Epoch:   9 |Batch: 160 |TotalExamples: 278050 |BatchLoss:   0.2276\n",
      "Epoch:   9 |Batch: 170 |TotalExamples: 278550 |BatchLoss:   0.1336\n",
      "Epoch:   9 |Batch: 180 |TotalExamples: 279050 |BatchLoss:   0.1520\n",
      "Epoch:   9 |Batch: 190 |TotalExamples: 279550 |BatchLoss:   0.1529\n",
      "Epoch:   9 |Batch: 200 |TotalExamples: 280050 |BatchLoss:   0.1085\n",
      "Epoch:   9 |Batch: 210 |TotalExamples: 280550 |BatchLoss:   0.1264\n",
      "Epoch:   9 |Batch: 220 |TotalExamples: 281050 |BatchLoss:   0.1516\n",
      "Epoch:   9 |Batch: 230 |TotalExamples: 281550 |BatchLoss:   0.1334\n",
      "Epoch:   9 |Batch: 240 |TotalExamples: 282050 |BatchLoss:   0.2171\n",
      "Epoch:   9 |Batch: 250 |TotalExamples: 282550 |BatchLoss:   0.1548\n",
      "Epoch:   9 |Batch: 260 |TotalExamples: 283050 |BatchLoss:   0.1511\n",
      "Epoch:   9 |Batch: 270 |TotalExamples: 283550 |BatchLoss:   0.1770\n",
      "Epoch:   9 |Batch: 280 |TotalExamples: 284050 |BatchLoss:   0.1200\n",
      "Epoch:   9 |Batch: 290 |TotalExamples: 284550 |BatchLoss:   0.1614\n",
      "Epoch:   9 |Batch: 300 |TotalExamples: 285050 |BatchLoss:   0.1151\n",
      "Epoch:   9 |Batch: 310 |TotalExamples: 285550 |BatchLoss:   0.2159\n",
      "Epoch:   9 |Batch: 320 |TotalExamples: 286050 |BatchLoss:   0.2084\n",
      "Epoch:   9 |Batch: 330 |TotalExamples: 286550 |BatchLoss:   0.1667\n",
      "Epoch:   9 |Batch: 340 |TotalExamples: 287050 |BatchLoss:   0.1663\n",
      "Epoch:   9 |Batch: 350 |TotalExamples: 287550 |BatchLoss:   0.1095\n",
      "Epoch:   9 |Batch: 360 |TotalExamples: 288050 |BatchLoss:   0.1808\n",
      "Epoch:   9 |Batch: 370 |TotalExamples: 288550 |BatchLoss:   0.1163\n",
      "Epoch:   9 |Batch: 380 |TotalExamples: 289050 |BatchLoss:   0.1437\n",
      "Epoch:   9 |Batch: 390 |TotalExamples: 289550 |BatchLoss:   0.1313\n",
      "Epoch:   9 |Batch: 400 |TotalExamples: 290050 |BatchLoss:   0.1946\n",
      "Epoch:   9 |Batch: 410 |TotalExamples: 290550 |BatchLoss:   0.2049\n",
      "Epoch:   9 |Batch: 420 |TotalExamples: 291050 |BatchLoss:   0.1787\n",
      "Epoch:   9 |Batch: 430 |TotalExamples: 291550 |BatchLoss:   0.1571\n",
      "Epoch:   9 |Batch: 440 |TotalExamples: 292050 |BatchLoss:   0.1929\n",
      "Epoch:   9 |Batch: 450 |TotalExamples: 292550 |BatchLoss:   0.1697\n",
      "Epoch:   9 |Batch: 460 |TotalExamples: 293050 |BatchLoss:   0.1778\n",
      "Epoch:   9 |Batch: 470 |TotalExamples: 293550 |BatchLoss:   0.1473\n",
      "Epoch:   9 |Batch: 480 |TotalExamples: 294050 |BatchLoss:   0.2334\n",
      "Epoch:   9 |Batch: 490 |TotalExamples: 294550 |BatchLoss:   0.1532\n",
      "Epoch:   9 |Batch: 500 |TotalExamples: 295050 |BatchLoss:   0.1957\n",
      "Epoch:   9 |Batch: 510 |TotalExamples: 295550 |BatchLoss:   0.2044\n",
      "Epoch:   9 |Batch: 520 |TotalExamples: 296050 |BatchLoss:   0.1573\n",
      "Epoch:   9 |Batch: 530 |TotalExamples: 296550 |BatchLoss:   0.1826\n",
      "Epoch:   9 |Batch: 540 |TotalExamples: 297050 |BatchLoss:   0.1542\n",
      "Epoch:   9 |Batch: 550 |TotalExamples: 297550 |BatchLoss:   0.1555\n",
      "Epoch:   9 |Batch: 560 |TotalExamples: 298050 |BatchLoss:   0.2369\n",
      "Epoch:   9 |Batch: 570 |TotalExamples: 298550 |BatchLoss:   0.1877\n",
      "Epoch:   9 |Batch: 580 |TotalExamples: 299050 |BatchLoss:   0.1433\n",
      "Epoch:   9 |Batch: 590 |TotalExamples: 299550 |BatchLoss:   0.1815\n",
      "Epoch Over:   9 |MeanEpochLoss:   0.1689 |ValidationSetLoss:   0.1670 \n",
      "\n",
      "Training network  lru_urnn  done.\n",
      "Init and training LRU for one timestep done.\n",
      "Initializing and training URNNs for one timestep...\n",
      "Network __init__ over. Number of trainable params= 3073\n",
      "Training network  lru_urnn ... timesteps= 200\n",
      "Starting training for lru_urnn\n",
      "NumEpochs:  10 |BatchSize:  50 |NumBatches:  1000 \n",
      "\n",
      "Epoch Starting: 0 \n",
      "\n",
      "Epoch:   0 |Batch:   0 |TotalExamples:    50 |BatchLoss:   1.1516\n",
      "Epoch:   0 |Batch:  10 |TotalExamples:   550 |BatchLoss:   0.1826\n",
      "Epoch:   0 |Batch:  20 |TotalExamples:  1050 |BatchLoss:   0.1669\n",
      "Epoch:   0 |Batch:  30 |TotalExamples:  1550 |BatchLoss:   0.1920\n",
      "Epoch:   0 |Batch:  40 |TotalExamples:  2050 |BatchLoss:   0.2271\n",
      "Epoch:   0 |Batch:  50 |TotalExamples:  2550 |BatchLoss:   0.1776\n",
      "Epoch:   0 |Batch:  60 |TotalExamples:  3050 |BatchLoss:   0.1678\n",
      "Epoch:   0 |Batch:  70 |TotalExamples:  3550 |BatchLoss:   0.2037\n",
      "Epoch:   0 |Batch:  80 |TotalExamples:  4050 |BatchLoss:   0.1773\n",
      "Epoch:   0 |Batch:  90 |TotalExamples:  4550 |BatchLoss:   0.1594\n",
      "Epoch:   0 |Batch: 100 |TotalExamples:  5050 |BatchLoss:   0.1367\n",
      "Epoch:   0 |Batch: 110 |TotalExamples:  5550 |BatchLoss:   0.1382\n",
      "Epoch:   0 |Batch: 120 |TotalExamples:  6050 |BatchLoss:   0.1422\n",
      "Epoch:   0 |Batch: 130 |TotalExamples:  6550 |BatchLoss:   0.1795\n",
      "Epoch:   0 |Batch: 140 |TotalExamples:  7050 |BatchLoss:   0.2531\n",
      "Epoch:   0 |Batch: 150 |TotalExamples:  7550 |BatchLoss:   0.2030\n",
      "Epoch:   0 |Batch: 160 |TotalExamples:  8050 |BatchLoss:   0.1748\n",
      "Epoch:   0 |Batch: 170 |TotalExamples:  8550 |BatchLoss:   0.1999\n",
      "Epoch:   0 |Batch: 180 |TotalExamples:  9050 |BatchLoss:   0.1035\n",
      "Epoch:   0 |Batch: 190 |TotalExamples:  9550 |BatchLoss:   0.1809\n",
      "Epoch:   0 |Batch: 200 |TotalExamples: 10050 |BatchLoss:   0.1485\n",
      "Epoch:   0 |Batch: 210 |TotalExamples: 10550 |BatchLoss:   0.1234\n",
      "Epoch:   0 |Batch: 220 |TotalExamples: 11050 |BatchLoss:   0.1615\n",
      "Epoch:   0 |Batch: 230 |TotalExamples: 11550 |BatchLoss:   0.2269\n",
      "Epoch:   0 |Batch: 240 |TotalExamples: 12050 |BatchLoss:   0.1432\n",
      "Epoch:   0 |Batch: 250 |TotalExamples: 12550 |BatchLoss:   0.1553\n",
      "Epoch:   0 |Batch: 260 |TotalExamples: 13050 |BatchLoss:   0.1610\n",
      "Epoch:   0 |Batch: 270 |TotalExamples: 13550 |BatchLoss:   0.1614\n",
      "Epoch:   0 |Batch: 280 |TotalExamples: 14050 |BatchLoss:   0.1779\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/These/Optimization of recurrent models/Code/adding_task/attraction_basins_adam.json/evals.py:160\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone and done.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 160\u001b[0m     main \u001b[38;5;241m=\u001b[39m Main()\n\u001b[1;32m    161\u001b[0m     main\u001b[38;5;241m.\u001b[39minit_data()\n\u001b[1;32m    162\u001b[0m     main\u001b[38;5;241m.\u001b[39mtrain_networks()\n",
      "File \u001b[0;32m~/Documents/These/Optimization of recurrent models/Code/adding_task/attraction_basins_adam.json/evals.py:149\u001b[0m, in \u001b[0;36mtrain_networks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_networks\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 149\u001b[0m     timesteps_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(timesteps_idx):\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_lru_for_timestep_idx(i)\n",
      "File \u001b[0;32m~/Documents/These/Optimization of recurrent models/Code/adding_task/attraction_basins_adam.json/evals.py:100\u001b[0m, in \u001b[0;36mtrain_lru_for_timestep_idx\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitializing and training URNNs for one timestep...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# AP\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39map_lru \u001b[38;5;241m=\u001b[39m TFRNN(\n\u001b[1;32m     89\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlru_urnn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     90\u001b[0m     num_in\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     91\u001b[0m     num_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     92\u001b[0m     num_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     93\u001b[0m     num_target\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     94\u001b[0m     single_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     95\u001b[0m     rnn_cell\u001b[38;5;241m=\u001b[39mLinearUnit,\n\u001b[1;32m     96\u001b[0m     activation_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# modReLU\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     activation_out\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m     98\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m params: optim\u001b[38;5;241m.\u001b[39mRMSprop(params, lr\u001b[38;5;241m=\u001b[39mglob_learning_rate, alpha\u001b[38;5;241m=\u001b[39mglob_decay),\n\u001b[1;32m     99\u001b[0m     loss_function\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m--> 100\u001b[0m )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# print('as init', self.ap_lru.cell.as_real + 1j * self.ap_lru.cell.as_imag)\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_network(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39map_lru, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39map_data[idx],\n\u001b[1;32m    103\u001b[0m                    \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39map_batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39map_epochs)\n",
      "File \u001b[0;32m~/Documents/These/Optimization of recurrent models/Code/adding_task/attraction_basins_adam.json/evals.py:55\u001b[0m, in \u001b[0;36mtrain_network\u001b[0;34m(self, net, dataset, batch_size, epochs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_network\u001b[39m(\u001b[38;5;28mself\u001b[39m, net, dataset, batch_size, epochs):\n\u001b[0;32m---> 55\u001b[0m     sample_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mget_sample_len())\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining network \u001b[39m\u001b[38;5;124m'\u001b[39m, net\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m... timesteps=\u001b[39m\u001b[38;5;124m'\u001b[39m, sample_len)\n\u001b[1;32m     57\u001b[0m     net\u001b[38;5;241m.\u001b[39mtrain(dataset, batch_size, epochs)\n",
      "File \u001b[0;32m~/Documents/These/Optimization of recurrent models/Code/adding_task/attraction_basins_adam.json/models/tf_rnn.py:137\u001b[0m, in \u001b[0;36mTFRNN.train\u001b[0;34m(self, dataset, batch_size, epochs)\u001b[0m\n\u001b[1;32m    134\u001b[0m Y_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(Y_batch, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# evaluate\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# save the loss for later\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_list\u001b[38;5;241m.\u001b[39mappend(batch_loss)\n",
      "File \u001b[0;32m~/Documents/These/Optimization of recurrent models/Code/adding_task/attraction_basins_adam.json/models/tf_rnn.py:199\u001b[0m, in \u001b[0;36mTFRNN.evaluate\u001b[0;34m(self, X, Y, training)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNew loss function\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m--> 199\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Rodeo/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Rodeo/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run evals.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rodeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
