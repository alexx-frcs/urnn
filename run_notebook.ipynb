{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Done.\n",
      "Starting training...\n",
      "Initializing and training URNNs for one timestep...\n",
      "Network __init__ over. Number of trainable params= 2102273\n",
      "Training network  lru_urnn ... timesteps= 100\n",
      "Starting training for lru_urnn\n",
      "NumEpochs:  10 |BatchSize:  50 |NumBatches:   600 \n",
      "\n",
      "Epoch Starting: 0 \n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected m1 and m2 to have the same dtype, but got: c10::complex<float> != float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Thèse/Optimization of recurrent models/Code/adding_task/attraction_basins_adam.json/evals.py:159\u001b[0m\n\u001b[1;32m    157\u001b[0m main \u001b[38;5;241m=\u001b[39m Main()\n\u001b[1;32m    158\u001b[0m main\u001b[38;5;241m.\u001b[39minit_data()\n\u001b[0;32m--> 159\u001b[0m \u001b[43mmain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_networks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Thèse/Optimization of recurrent models/Code/adding_task/attraction_basins_adam.json/evals.py:148\u001b[0m, in \u001b[0;36mMain.train_networks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m timesteps_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(timesteps_idx):\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_lru_for_timestep_idx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(timesteps_idx):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_urnn_for_timestep_idx(i)\n",
      "File \u001b[0;32m~/Documents/Thèse/Optimization of recurrent models/Code/adding_task/attraction_basins_adam.json/evals.py:99\u001b[0m, in \u001b[0;36mMain.train_lru_for_timestep_idx\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# AP\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39map_lru \u001b[38;5;241m=\u001b[39m TFRNN(\n\u001b[1;32m     87\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlru_urnn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     88\u001b[0m     num_in\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m     loss_function\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m     98\u001b[0m )\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43map_lru\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43map_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43map_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43map_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInit and training URNNs for one timestep done.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Thèse/Optimization of recurrent models/Code/adding_task/attraction_basins_adam.json/evals.py:55\u001b[0m, in \u001b[0;36mMain.train_network\u001b[0;34m(self, net, dataset, batch_size, epochs)\u001b[0m\n\u001b[1;32m     53\u001b[0m sample_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mget_sample_len())\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining network \u001b[39m\u001b[38;5;124m'\u001b[39m, net\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m... timesteps=\u001b[39m\u001b[38;5;124m'\u001b[39m, sample_len)\n\u001b[0;32m---> 55\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# loss_list has one number for each batch (step)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m serialize_loss(net\u001b[38;5;241m.\u001b[39mget_loss_list(), net\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m sample_len)\n",
      "File \u001b[0;32m~/Documents/Thèse/Optimization of recurrent models/Code/adding_task/attraction_basins_adam.json/models/tf_rnn.py:136\u001b[0m, in \u001b[0;36mTFRNN.train\u001b[0;34m(self, dataset, batch_size, epochs)\u001b[0m\n\u001b[1;32m    133\u001b[0m Y_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(Y_batch, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# evaluate\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# save the loss for later\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_list\u001b[38;5;241m.\u001b[39mappend(batch_loss)\n",
      "File \u001b[0;32m~/Documents/Thèse/Optimization of recurrent models/Code/adding_task/attraction_basins_adam.json/models/tf_rnn.py:177\u001b[0m, in \u001b[0;36mTFRNN.evaluate\u001b[0;34m(self, X, Y, training)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m outputs_o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# compute loss\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function, \u001b[38;5;28mtype\u001b[39m(nn\u001b[38;5;241m.\u001b[39mMSELoss())):\n",
      "File \u001b[0;32m~/Documents/Thèse/Optimization of recurrent models/Code/adding_task/attraction_basins_adam.json/models/tf_rnn.py:90\u001b[0m, in \u001b[0;36mTFRNN.forward\u001b[0;34m(self, input_x)\u001b[0m\n\u001b[1;32m     88\u001b[0m         output, state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell(input_t, state)\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m         output, state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     outputs_h\u001b[38;5;241m.\u001b[39mappend(output\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# [batch_size, 1, output_size]\u001b[39;00m\n\u001b[1;32m     92\u001b[0m outputs_h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(outputs_h, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size, max_time, output_size]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Rodeo/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Rodeo/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Thèse/Optimization of recurrent models/Code/adding_task/attraction_basins_adam.json/models/lru_cell.py:100\u001b[0m, in \u001b[0;36mLinearUnit.forward\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m     94\u001b[0m state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mcomplex64)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# print('dtype B', self.B_real.dtype)\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# print('dtype inputs', inputs.dtype)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# print('dtype B', self.B_imag.dtype)\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m inputs_mul \u001b[38;5;241m=\u001b[39m (\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mB_real\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mcomplex64) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39mj \u001b[38;5;241m*\u001b[39m inputs \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB_imag\u001b[38;5;241m.\u001b[39mt())\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mcomplex64) \u001b[38;5;66;03m# [batch_sz, num_units]\u001b[39;00m\n\u001b[1;32m    102\u001b[0m state_c \u001b[38;5;241m=\u001b[39m state[:, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_units] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39mj \u001b[38;5;241m*\u001b[39m state[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_units:]  \u001b[38;5;66;03m# [batch_sz, num_units]\u001b[39;00m\n\u001b[1;32m    104\u001b[0m state_mul \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA_real\u001b[38;5;241m.\u001b[39mmul(state_c) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39mj \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA_imag\u001b[38;5;241m.\u001b[39mmul(state_c) \u001b[38;5;66;03m# [batch_sz, num_units]\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected m1 and m2 to have the same dtype, but got: c10::complex<float> != float"
     ]
    }
   ],
   "source": [
    "%run evals.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rodeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
